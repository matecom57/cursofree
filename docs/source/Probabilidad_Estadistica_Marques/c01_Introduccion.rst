TEMA:  Introducción
================

1.1 Introducción y Definición de Estadística
--------------------------------------------

El término **estadística** se derivó originalmente del vocablo "**estado**", porque ha sido función tradicional de los 
gobiernos centrales llevar registros de población, nacimiento, defunciones, profesiones, cosechas y muchas otras 
clases de cosas y actividades. Contar y medir estos hechos genera muchas clases de datos numéricos.

**Definición tradicional de estadística**: colección, organización, resumen y presentación de datos numéricos.

Actualmente la Estadística es una rama de la matemática aplicada que colecciona, clasifica y evalúa o analiza datos 
como base para hacer inferencias o conclusiones válidas, así como para tomar decisiones en base a ese análisis.

Como un procedimiento de toma de decisiones, la Estadística es de importancia creciente en varios campos, 
por ejemplo, en la producción industrial en masa, Medicina, Biología, Economía, Política, Psicología, 
análisis de opinión pública, Agricultura, Meteorología, Física, Química e Ingeniería.

1.2 Relacion entre Estadistica y Probabilidad
---------------------------------------------

La **Probabilidad** es el estudio de fenómenos puramente aleatorios mientras que la Estadística se puede describir 
como 
la ciencia o el arte de reunir y analizar datos e inferir consecuencias a partir de estos elementos. Como el azar afecta tanto a la reunión de datos como a su análisis, y se debe tener en cuenta al hacer inferencias, el tener conocimientos en estadística implica poseer una buena base sobre la teoría de la probabilidad.

1.3 Síntesis de la Estadistica y de la Probabilidad
---------------------------------------------------

La **Teoría de la Probabilidad** tuvo su origen en las apuestas en **juegos de azar**; a Gerolamo Cardano (1501-1576), 
físico, astrónomo y maternatico, se le atribuye la primera discusión sobre probabilidad en su manual para jugadores; 
pero fueron **Blaise Pascal** (1623-1662) y **Pierre de Fermat** (1600-1665), alrededor de la primera mitad del 
**siglo XVII** quienes desarrollaron la **Teoría de la Probabilidad**,

**Jakob Bernoulli** (1654-1705) fue también uno de los primeros que estudiaron la probabilidad matemática; 
su nombre va asociado a varios conceptos matemáticos, como los **Experimentos de Bernoulli** en probabilidad.

La **curva normal** ha sido de gran importancia en el desarrollo de la estadística. La ecuación de esta curva 
fue publicada por primera vez en **1733** por **Abraham De Moivre**. Pero De Moivre no tenia idea de su aplicación 
en observaciones experimentales y su publicación era desconocida hasta que **Karl Pearson** la encontró 
en una Biblioteca en **1924**. Sin embargo, esta ecuación también fue descubierta posteriormente por dos 
astrónomos y matemáticos: **Laplace** (1749-1827) y **Gauss** (1777-1855) independientemente, 
hoy en día se conoce también como la curva de Gauss o campana de Gauss.

En el siglo XIX **Charles Lyell** encontró aplicación de la Estadística a un problema de Geología. 
Entre 1830 y 1833 Lyell publicó tres volúmenes sobre Geología donde establece la relación entre 
las rocas terciarias y sus respectivos nombres. El razonamiento de Lyell fue esencialmente estadístico. 

**Charles Darwin** (1809-1882), biólogo, recibió el segundo volumen de Lyell y se cree que su teoría 
fue influenciada por este libro. El trabajo de Darwin fue principalmente de naturaleza biométrica o estadística. 
También **Gregor Mendel** con su estudio de las plantas híbridas publicado en 1866 tenía un problema de tipo 
biométrico o estadístico.

Pafhuti Lvovich **Chevyshev** (1821-1894) contribuyó a la teoría de la probabilidad con la **Desigualdad de 
Chevyshev**, que debido a su generalidad resulta ser una herramienta teórica muy importante.

En el siglo XIX la necesidad de una profundización en las bases de la Estadística se hizo trascendental, 
**Karl Pearson** (1857-1936) físico-matemático inglés, inspirado en Darwin, aplicó sus matemáticas a la evolución. 
Pearson, considerado el **padre de la Estadística** pasó casi medio siglo haciendo una profunda 
investigación en Estadística, asi también fue el fundador de la revista "**Biometrika**" y de la 
Escuela de Estadística en Cambridge, Inglaterra ganando gran ímpetu el estudio de esta materia. 
A Karl Pearson se debe el estudio de la **bondad de ajuste** con la **distribución** $X^2$ y el **coeficiente de 
correlación** entre dos variables.

Mientras Karl Pearson trabajaba con grandes muestras, la teoría de las grandes muestras era inadecuada 
para los investigadores que tenían que trabajar con pequeñas muestras. Entre ellos estaba 
**W. S. Gosset** (1876- 1937), alumno de K. Pearson y científico de la Cervecería "**Guinness**". 
Los conocimientos matemáticos de Gosset mostraron haber sido insuficientes para el reto de 
encontrar distribuciones exactas de la desviación estándar de la muestra, del cociente de la media 
y la desviación estándar de una muestra y del coeficiente de correlación, estadígrafos con los 
cuales él comunmente trabajaba. Consecuentemente el recurrió a compilar y computar las 
distribuciones de frecuencias empíricas al tomar cartas de un paquete de cartas barajadas. 
Los resultados de estos trabajos aparecieron en la re vista "Biometrika" en **1908** bajo 
el seudónimo de "**student**". Hoy en día, la distribución **" t de estudent** es una herramienta básica para los 
estadísticos y experimentadores. Ahora que el uso de la distribución t de student está mundialmente difundida 
es interesante notar que el astrónomo alemán Helmet había obtenido los mismos resultados teóricamente a 
principios de **1875**.

**Ronald Alymer Fisher** (1890-1962), especialista inglés en Genética y Estadística, fue influenciado por Karl 
Pearson y Gosset; hizo numerosas e importantes contribuciones a la Estadística, precisó métodos estádisticos 
para interpretar datos cuantitativos. En su trabajo sobre pruebas de hipótesis, desarrolló aplicaciones de 
la **distribución F**, por lo que lleva su nombre. Esta distribución se utiliza para probar hipótesis acerca 
de dos varíanzas de pequeñas muestras. También la **Z de Fisher** usada para probar hipótesis acerca del 
coeficiente de correlación lineal.

**Jerzy Neyman** (1894- ) y E. S. **Pearson** (1895- ) presentaron una teoría de pruebas de hipótesis 
estadísticas en 
1936 y 1938; esta teoría promovió considerablemente la investigación y, muchos de sus resultados son de 
gran utilidad práctica.

**William Feiler**, nacido en 1906, contribuyó a la teoría de la probabilidad con su trabajo sobre el 
**Teorema del Limite Central** y las **cadenas de Markov**. Introdujo un nuevo tratamiento en su libro 
"An Introduction to Probability and its Aplications" (1961) que contiene muchos ejemplos que explican 
nuevas aplicaciones a los fenómenos biológicos, físicos y estadisticos.

**John von Neumann** (1909-1957) llevó a cabo la primera demostración del **teorema minimax**, 
base fundamental de la teoría de juegos, que fue propuesto primeramente por **Emile Borel** en 1921. 
También fue un pionero de la teoría de las computadoras, habiendo diseñado y construido el 
llamado **MANIAC** (analizador matemático, integrador numérico y computador) en el Instituto para Estudios Avanzados 
en Princeton en 1952.

**Abraham Wald** (1902-1950) en sus dos libros "Sequential Analysis" y "Statistical Decision Fuctions" alcanzó 
grandes logros en Estadística y sus aplicaciones.

Así, en este siglo es cuando se han desarrollado la mayoría de los métodos estadísticos que se usan en la actualidad,

1.4 Estadística e Investigación
-------------------------------

La Estadística interviene en la investigación y/o el método científico, a través de la experimentación y observación. 
Esto es, las observaciones experimentales y conocimientos son partes integrantes del método científico y esos métodos 
invariablemente conducen al empleo de técnicas de la Estadística. Ya que la Estadística, cuando se usa adecuadamente, 
hace más eficientes las investigaciones, es aconsejable que los investigadores se familiaricen con las técnicas 
y conceptos básicos de esta ciencia tan útil.

El uso de la estadística como herramienta de la investigación no puede separarse de la planeación general 
del proyecto de investigación. Si un proyecto de investigación debe producir datos que van a ser tratados 
estadísticamente, entonces un método estadístico apropiado debe formar una parte integrante del diseño total. 


1.5 Etapas de ina Investigacion Estadistica
-------------------------------------------

1. **Formulación del problema**: Para investigar con éxito un problema dado, primero tenemos que crear conceptos 
precisos, formular preguntas claras, e imponer limitaciones adecuadas al problema, tomando en cuenta el tiempo y 
el dinero disponibles y la habilidad de los investigadores. Si se fracasa en esta formulación, 
los datos compilados pueden ser irrelevantes o inadecuados.

Es bueno rocordar que la calidad de las conclusiones estadísticas depende de la corrección y precisión 
de los datos que, a su vez, dependen de la exactitud en la formulación del prohlema. Las técnicas 
estadísticas, por muy refinadas y precisas que sean, no pueden ayudar a alcazar decisiones si son 
aplicadas a datos inapropiados.

2. **Diseño del experimento**. Nuestro deseo es obtener un máximo de información empleando un mínimo de costo y 
tiempo. Esto implica, entre otras cosas, que debemos determinar el tamaño de muestra, o la cantidad y 
tipo de datos que resolverán más eficientemente el problema. 

Obtener una muestra representativa es fundamental en teoría estadística. Supone preguntas como estas: 
¿Qué tipo de datos debe recogerse? ¿Cómo deben ser compilados los datos? ¿De qué tamaño debe ser la muestra? 
Estas preguntas corresponden a lo que se conoce como diseño de muestras o diseño experimental. 
Debe tenerse cuidado al planificar y diseñar un experimento; de otro modo, puede que no lleguemos a alcanzar ninguna 
conclusión válida.

3. **Colección de datos y experimentación**: La compilación de datos se refiere a los métodos usados para 
obtener información pertinente de las unidades elementales introducidas en una muestra. 
En general, ésta es la parte que más tiempo consume en toda investigación que sea realizada. Esta debe sujetarse a 
reglas estrictas.

4. **Tabulación y descripción de los resultados**: En esta etapa los datos experimentales deben ser ordenados 
en forma legible y se ilustran con representaciones gráficas (diagramas o gráficas ); además se calculan 
medidas descriptivas para el tamaño promedio y la separación o dispersión de los valores de la muestra. 

5. **Ingererencia estadística formulación de la respuesta**: Al aplicar el método estadístico seleccionado,
 obtenemos 
conclusiones a partir de la muestra, acerca de la población correspondiente (inferencia estadística ), 
tomamos una decisión y formulamos la respuesta a nuestro problema.

No existe una fórmula mágica en estadística matemática que tome en cuenta todas las 
situaciones prácticas concebibles. Por lo cual es necesario adquirir conocimientos generales de los 
métodos más importantes que sean útiles para hacer inferencias. En cada caso práctico debe 
estudiarse con cuidado la naturaleza del problema específico, para estar seguros de que será 
escogido el método más apropiado.

1.6 Estadistica Descriptiva y Estadistica Inferencial
-----------------------------------------------------

Los datos tal como se obtienen no nos proporcionan información suficiente para interpretar su 
significado por lo que tenemos que utilizar métodos descriptivos para darles mayor sentido o 
inferenciales para sacar conclusiones válidas sobre ellos. Estos métodos dependen del tipo de 
datos que se tengan y de los resultados que se quieran obtener.

Los métodos descriptivos se emplean para esquematizar o mostrar los datos en forma ordenada y gráfica 
sin sacar conclusiones de ellos. Los métodos descriptivos se pueden usar tanto para muestras como para 
poblaciones mientras que los métodos inferenciales usan solamente muestras para inferir a partir de las primeras, 
las características de la población. Cuando usamos Estadística Inferencial generalizamos a partir de las 
características de una muestra.

1.7 Poblacion y Muestra
-----------------------

Una población o Universo es un agregado o la totalidad de unidades elementales tales como personas, 
empresas industriales, granjas o datos de cualquier clase acerca de los cuales se desea información. 
Una muestra es una porción o subconjunto de unidades elementales extraídas de una población

1.8 Unidades Elementales y Observación
--------------------------------------

Los individuos u objetos de una población que tienen una característica medible se llaman unidades elementales; 
definir una población es, en un sentido, limitar el contenido de las unidades elementales. 
Estas poseen ciertas características, conocidas a veces como rasgos o propiedades, que pueden ser 
de naturaleza cualitativa o cuantitativa.

El término observación se usará para indicar cualquier clase de medida obtenida en la investigación, 
es decir, el resultado de observar o medir una unidad elemental, se llama observación; 
también se puede entender como el valor numérico de una característica cuantificable de una unidad elemental.

1.9 Sumatorias
--------------

Dado un conjunto de observaciones de alguna variable representada por :math:`X_1, X_2,....,X_n`, podemos expresar su suma $X_1+X_2+....+X_n$ en forma abre- viada como

.. math::
   
   \sum_{i=1}^n X_i


Esto se lee " suma de los :math:`X_i` desde i igual a 1 hasta n"

Ejemplo 1.1. Si :math:`X_1=1, X_2=-3, X_3=\frac{1}{4}`, entonces

.. math::

   \sum_{i=1}^3 X_i = X_1 + X_2 + X_3 = 1+(-3) + \frac{1}{4} = - \frac{7}{4}


**Propiedades de las Sumatorias**

1.  Si c es una constante cualquiera, entonces :math:`\sum_{i=1}^n cX_i = c \sum_{i=1}^nX_i`

**Corolario**: Si c es una constante, entonces :math:`\sum_{i=1}^n c = nc`

2. :math:`\sum\_{i=1}\^n (X_i + Y_i + Z_i) = \\sum\_{i=1}\^n X_i  + \\sum\_{i=1}\^n Y_i  + \\sum\_{i=1}\^n Z_i`

Colorario: :math:`\sum_{i=1}^n (X_i + C) = \sum_{i=1}^n X_i + nC`

Sumatorias Dobles:

Frecuentemente en estadística se desea cono- cer la interacción entre dos variables; asi por ejemplo, consideramos las 20 determinaciones de presión sanguínea sistólica tomadas a un individuo que participa en un programa ideado para estudiar fuentes e intensidades de variación de lecturas de la presión de la sangre. La presión de la san gre fue medida por 4 médicos en cada una de 5 visitas. Los datos se resu- men en la siguiente tabla.
